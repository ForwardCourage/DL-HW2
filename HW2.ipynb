{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUw7BAcveuHS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4mL6h1WexBX",
        "outputId": "6cecb11e-ea98-4026-95b5-faf501c62483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.chdir(\"/content/gdrive/MyDrive\")\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xYQ3NyN_e1y-",
        "outputId": "df1ca808-09a3-41b4-ef6a-e6743a8e9f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the .npy files to load the arrays\n",
        "with open(\"train_file.npy\", \"rb\") as f:\n",
        "  train = np.load(f)\n",
        "\n",
        "with open(\"valid_file.npy\", \"rb\") as f:\n",
        "  valid = np.load(f)\n",
        "\n",
        "with open(\"test_file.npy\", \"rb\") as f:\n",
        "  test = np.load(f)\n",
        "\n",
        "train = train.reshape(63325, 1, 100, 100)\n",
        "valid = valid.reshape(450, 1, 100, 100)\n",
        "test = test.reshape(450, 1, 100, 100)"
      ],
      "metadata": {
        "id": "tpNCpF4we2ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading for the labels\n",
        "train_labels, valid_labels, test_labels = [], [], []\n",
        "\n",
        "with open(\"train.txt\", \"r\") as f:\n",
        "  lines = [line.rstrip() for line in f]\n",
        "\n",
        "for line in lines:\n",
        "  s = line.split(\" \")\n",
        "  train_labels.append(int(s[1]))\n",
        "\n",
        "with open(\"test.txt\", \"r\") as f:\n",
        "  lines = [line.rstrip() for line in f]\n",
        "\n",
        "for line in lines:\n",
        "  s = line.split(\" \")\n",
        "  test_labels.append(int(s[1]))\n",
        "\n",
        "with open(\"val.txt\", \"r\") as f:\n",
        "  lines = [line.rstrip() for line in f]\n",
        "\n",
        "for line in lines:\n",
        "  s = line.split(\" \")\n",
        "  valid_labels.append(int(s[1]))\n",
        "\n",
        "train_labels = np.eye(50)[train_labels]\n",
        "valid_labels = np.eye(50)[valid_labels]\n",
        "test_labels = np.eye(50)[test_labels]"
      ],
      "metadata": {
        "id": "xfkh4amye_t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(valid.shape)\n",
        "print(test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hQMINVafIH7",
        "outputId": "64b54970-e065-4531-daba-3c8a3c09ae16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63325, 1, 100, 100)\n",
            "(450, 1, 100, 100)\n",
            "(450, 1, 100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=train.astype(\"float32\")/255\n",
        "valid=valid.astype(\"float32\")/255\n",
        "test=test.astype(\"float32\")/255"
      ],
      "metadata": {
        "id": "mV-allVvpiGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D:\n",
        "  def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride, learning_rate, name):\n",
        "    # weight size: (F, C, K, K)\n",
        "    # bias size: (F) \n",
        "    self.F = num_filters\n",
        "    self.K = kernel_size\n",
        "    self.C = inputs_channel\n",
        "\n",
        "    self.weights = np.zeros((self.F, self.C, self.K, self.K))\n",
        "    self.bias = np.zeros((self.F, 1))\n",
        "    for i in range(0,self.F):\n",
        "        self.weights[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))\n",
        "\n",
        "    self.p = padding\n",
        "    self.s = stride\n",
        "    self.lr = learning_rate\n",
        "    self.name = name\n",
        "\n",
        "  def zero_padding(self, inputs, size):\n",
        "    w, h = inputs.shape[0], inputs.shape[1]\n",
        "    new_w = 2 * size + w\n",
        "    new_h = 2 * size + h\n",
        "    out = np.zeros((new_w, new_h))\n",
        "    out[size:w+size, size:h+size] = inputs\n",
        "    return out\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # input size: (C, W, H)\n",
        "    # output size: (N, F ,WW, HH)\n",
        "    C = inputs.shape[0]\n",
        "    W = inputs.shape[1]+2*self.p\n",
        "    H = inputs.shape[2]+2*self.p\n",
        "    self.inputs = np.zeros((C, W, H))\n",
        "    for c in range(inputs.shape[0]):\n",
        "        self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)\n",
        "    WW = int((W - self.K)/self.s + 1)\n",
        "    HH = int((H - self.K)/self.s + 1)\n",
        "    feature_maps = np.zeros((self.F, WW, HH))\n",
        "    for f in range(self.F):\n",
        "      for w in range(0, WW, self.s):\n",
        "        for h in range(0, HH, self.s):\n",
        "          feature_maps[f,w,h]=np.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]\n",
        "\n",
        "    return feature_maps\n",
        "\n",
        "  def backward(self, dy):\n",
        "\n",
        "    C, W, H = self.inputs.shape\n",
        "    dx = np.zeros(self.inputs.shape)\n",
        "    dw = np.zeros(self.weights.shape)\n",
        "    db = np.zeros(self.bias.shape)\n",
        "\n",
        "    F, W, H = dy.shape\n",
        "    for f in range(F):\n",
        "      for w in range(0, W, self.s):\n",
        "        for h in range(0, H, self.s):\n",
        "          dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "          dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
        "\n",
        "    for f in range(F):\n",
        "      db[f] = np.sum(dy[f, :, :])\n",
        "\n",
        "    self.weights -= self.lr * dw\n",
        "    self.bias -= self.lr * db\n",
        "    return dx\n",
        "\n",
        "  def extract(self):\n",
        "    return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "  def feed(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "class Maxpooling2D:\n",
        "\n",
        "  def __init__(self, pool_size, stride, name):\n",
        "    self.pool = pool_size\n",
        "    self.s = stride\n",
        "    self.name = name\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    C, W, H = inputs.shape\n",
        "    if (W%2) == 1 and (H%2) == 1:\n",
        "      input_alt = np.zeros((C,W+1,H+1))\n",
        "      input_alt[:,0:W,0:H] = inputs\n",
        "      self.inputs = input_alt\n",
        "      C, W, H = input_alt.shape\n",
        "      self.org_odd = True\n",
        "\n",
        "    else:\n",
        "      self.inputs = inputs\n",
        "      self.org_odd = False\n",
        "\n",
        "    new_width = int((W - (self.pool-1)-1)/self.s + 1)\n",
        "    new_height = int((H - (self.pool-1)-1)/self.s + 1)\n",
        "    out = np.zeros((C, new_width, new_height))\n",
        "    for c in range(C):\n",
        "      for w in range(new_width):\n",
        "        for h in range(new_height):\n",
        "          out[c, w, h] = np.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])\n",
        "    return out\n",
        "\n",
        "  def backward(self, dy):\n",
        "    C, W, H = self.inputs.shape\n",
        "    dx = np.zeros(self.inputs.shape)\n",
        "    for c in range(C):\n",
        "      for w in range(0, W, self.pool):\n",
        "        for h in range(0, H, self.pool):\n",
        "          st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "          (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
        "          dx[c, w+idx, h+idy] = dy[c, int(w/self.pool), int(h/self.pool)]\n",
        "    if self.org_odd == True:\n",
        "      dx = dx[:,0:(W-1),0:(H-1)]\n",
        "\n",
        "    return dx\n",
        "\n",
        "  def extract(self):\n",
        "    return \n",
        "\n",
        "class Dense:\n",
        "\n",
        "  def __init__(self, num_inputs, num_outputs, learning_rate, name):\n",
        "    self.weights = 0.01*np.random.rand(num_inputs, num_outputs)\n",
        "    self.bias = np.zeros((num_outputs, 1))\n",
        "    self.lr = learning_rate\n",
        "    self.name = name\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(self.inputs, self.weights) + self.bias.T\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dy):\n",
        "\n",
        "    if dy.shape[0] == self.inputs.shape[0]:\n",
        "        dy = dy.T\n",
        "    dw = dy.dot(self.inputs)\n",
        "    db = np.sum(dy, axis=1, keepdims=True)\n",
        "    dx = np.dot(dy.T, self.weights.T)\n",
        "\n",
        "    self.weights -= self.lr * dw.T\n",
        "    self.bias -= self.lr * db\n",
        "\n",
        "    return dx\n",
        "\n",
        "  def extract(self):\n",
        "    return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
        "\n",
        "  def feed(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "class Flatten:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self, inputs):\n",
        "    self.C, self.W, self.H = inputs.shape\n",
        "    return inputs.reshape(1, self.C*self.W*self.H)\n",
        "  def backward(self, dy):\n",
        "    return dy.reshape(self.C, self.W, self.H)\n",
        "  def extract(self):\n",
        "    return\n",
        "\n",
        "class ReLu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.outputs = np.maximum(0, inputs)\n",
        "    return self.outputs\n",
        "  def backward(self, dy):\n",
        "    dx = np.where(self.inputs > 0, dy, 0)\n",
        "    return dx\n",
        "  def extract(self):\n",
        "    return\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.outputs = inputs/(1+np.exp(-inputs))\n",
        "    return self.outputs\n",
        "  def backward(self, dy):\n",
        "    f_x = self.outputs\n",
        "    f_prime_x = (1 / (1 + np.exp(-self.inputs))) - self.x * np.exp(-self.inputs) / (1 + np.exp(-self.inputs))**2\n",
        "    dx = dy * f_prime_x\n",
        "    return dx\n",
        "  def extract(self):\n",
        "    pass\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  def forward(self, inputs):\n",
        "      exp = np.exp(inputs, dtype=\"float32\")\n",
        "      self.out = exp/np.sum(exp)\n",
        "      return self.out\n",
        "  def backward(self, dy):\n",
        "      return self.out.T - dy.reshape(dy.shape[0],1)\n",
        "  def extract(self):\n",
        "      return\n",
        "    \n",
        "def cross_entropy(inputs, labels):\n",
        "\n",
        "  out_num = labels.shape[0]\n",
        "  p = np.sum(labels.reshape(1,out_num)*inputs)\n",
        "  loss = -np.log(p+0.0000001)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "0khd2TGDfJ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "  def __init__(self):\n",
        "    lr = 0.01\n",
        "    self.layers = []\n",
        "    self.layers.append(Dense(num_inputs=10000, num_outputs=512, learning_rate=lr, name='fc1'))\n",
        "    self.layers.append(ReLU())\n",
        "    self.layers.append(Dense(num_inputs=512, num_outputs=50, learning_rate=lr, name='fc2'))\n",
        "    self.layers.append(Softmax())\n",
        "    self.lay_num = len(self.layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = x\n",
        "    for i in range(self.lay_num):\n",
        "      output = self.layers[i].forward(output)\n",
        "    return output\n",
        "\n",
        "  def backward(self, y):\n",
        "    for i in range(self.lay_num - 1, -1, -1):\n",
        "      y = self.layers[i].backward(y)\n",
        "    return y#actually not needed\n",
        "\n",
        "  def predict(self, data, label):\n",
        "    total_acc = 0\n",
        "    data_size = data.shape[0]\n",
        "    for i in range(data_size):\n",
        "      x = data[i]\n",
        "      y = label[i]\n",
        "      for l in range(self.lay_num):\n",
        "          output = self.layers[l].forward(x)\n",
        "          x = output\n",
        "      if np.argmax(output) == np.argmax(y):\n",
        "          total_acc += 1\n",
        "    return float(total_acc)\n",
        "\n",
        "  def extract(self, weights_file):\n",
        "    # dump weights and bias\n",
        "    obj = []\n",
        "    for i in range(self.lay_num):\n",
        "      cache = self.layers[i].extract()\n",
        "      obj.append(cache)\n",
        "    with open(weights_file, 'wb') as handle:\n",
        "      pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def train(self, training_data, training_label, batch_size, epoch, weights_file = None):\n",
        "    self.acc = []\n",
        "    self.vacc = []\n",
        "    total_acc = 0\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(training_data, training_label, test_size=0.2, stratify=training_label)\n",
        "\n",
        "    for e in range(epoch):\n",
        "      for batch_index in range(0, X_train.shape[0], batch_size):#taking datas <= batch size\n",
        "        if batch_index + batch_size < X_train.shape[0]:\n",
        "          data = X_train[batch_index:batch_index+batch_size]\n",
        "          label = training_label[batch_index:batch_index + batch_size]\n",
        "        else:\n",
        "          data = X_train[batch_index:X_train.shape[0]]\n",
        "          label = training_label[batch_index:training_label.shape[0]]\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        dy = []\n",
        "        for b in range(batch_size):#use the batch to forward\n",
        "          x = data[b]\n",
        "          y = label[b]\n",
        "          output = self.forward(x)\n",
        "          loss += cross_entropy(output, y)\n",
        "          if np.argmax(output) == np.argmax(y):\n",
        "              acc += 1\n",
        "              total_acc += 1\n",
        "          dy.append(y)\n",
        "        for y in dy:\n",
        "          self.backward(y)\n",
        "        # result\n",
        "        loss /= batch_size\n",
        "      \n",
        "  \n",
        "      training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))      \n",
        "      valid_acc = self.predict(X_valid, y_valid)\n",
        "      self.acc.append(training_acc)\n",
        "      self.vacc.append(valid_acc)\n",
        "      print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} ====== TAcc: {4:.2f} ====== VAcc: {5:.2f} ======'.format(e,epoch,batch_index+batch_size,loss,training_acc, valid_acc))\n",
        "\n",
        "    if (weights_file is not None):\n",
        "      self.extract(weights_file)\n",
        "\n",
        "  def acc_plot(self):\n",
        "    epochs = range(1, len(self.acc) + 1)\n",
        "\n",
        "    # 畫出兩條線\n",
        "    plt.plot(epochs, self.acc, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, self.vacc, 'r', label='Validation accuracy')\n",
        "\n",
        "    # 設置標題、x軸和y軸的標籤\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "C-K-Ng6SV2uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NN()\n",
        "\n",
        "train_nn = train.reshape(63325, 1, 10000)\n",
        "valid_nn = valid.reshape(450, 1, 10000)\n",
        "test_nn = test.reshape(450, 1, 10000)\n",
        "\n",
        "nn.train(train_nn, train_labels, 128, 5)\n",
        "nn.acc_plot()\n",
        "\n",
        "print(nn.predict(valid_nn, valid_labels))\n",
        "print(nn.predict(test_nn, test_labels))"
      ],
      "metadata": {
        "id": "xR-OvIO2ZPJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet5:\n",
        "  def __init__(self):\n",
        "    lr = 0.01\n",
        "    self.layers = []\n",
        "    self.layers.append(Conv2D(inputs_channel=1, num_filters=6, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv1'))\n",
        "    self.layers.append(ReLu())\n",
        "    self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))\n",
        "    self.layers.append(Conv2D(inputs_channel=6, num_filters=16, kernel_size=5, padding=0, stride=1, learning_rate=lr, name='conv3'))\n",
        "    self.layers.append(ReLu())\n",
        "    self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool4'))\n",
        "    self.layers.append(Conv2D(inputs_channel=16, num_filters=120, kernel_size=22, padding=0, stride=1, learning_rate=lr, name='conv5'))\n",
        "    self.layers.append(ReLu())\n",
        "    self.layers.append(Flatten())\n",
        "    self.layers.append(Dense(num_inputs=120, num_outputs=84, learning_rate=lr, name='fc6'))\n",
        "    self.layers.append(ReLu())\n",
        "    self.layers.append(Dense(num_inputs=84, num_outputs=50, learning_rate=lr, name='fc7'))\n",
        "    self.layers.append(Softmax())\n",
        "    self.lay_num = len(self.layers)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = x\n",
        "    for i in range(self.lay_num):\n",
        "      output = self.layers[i].forward(output)\n",
        "    return output\n",
        "\n",
        "  def backward(self, y):\n",
        "    for i in range(self.lay_num - 1, -1, -1):\n",
        "      y = self.layers[i].backward(y)\n",
        "    return y#actually not needed\n",
        "\n",
        "  def predict(self, data, label):\n",
        "    total_acc = 0\n",
        "    data_size = data.shape[0]\n",
        "    for i in range(data_size):\n",
        "      x = data[i]\n",
        "      y = label[i]\n",
        "      for l in range(self.lay_num):\n",
        "          output = self.layers[l].forward(x)\n",
        "          x = output\n",
        "      if np.argmax(output) == np.argmax(y):\n",
        "          total_acc += 1\n",
        "    return float(total_acc)\n",
        "    #print(\"Accuracy: \", float(total_acc)/float(data_size))\n",
        "\n",
        "  def extract(self, weights_file):\n",
        "    # dump weights and bias\n",
        "    obj = []\n",
        "    for i in range(self.lay_num):\n",
        "      cache = self.layers[i].extract()\n",
        "      obj.append(cache)\n",
        "    with open(weights_file, 'wb') as handle:\n",
        "      pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def train(self, training_data, training_label, batch_size, epoch, weights_file = None):\n",
        "    self.acc = []\n",
        "    self.vacc = []\n",
        "    total_acc = 0\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(training_data, training_label, test_size=0.2, stratify=training_label)\n",
        "\n",
        "    for e in range(epoch):\n",
        "      for batch_index in range(0, X_train.shape[0], batch_size):#taking datas <= batch size\n",
        "        if batch_index + batch_size < X_train.shape[0]:\n",
        "          data = X_train[batch_index:batch_index+batch_size]\n",
        "          label = training_label[batch_index:batch_index + batch_size]\n",
        "        else:\n",
        "          data = X_train[batch_index:X_train.shape[0]]\n",
        "          label = training_label[batch_index:training_label.shape[0]]\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        dy = []\n",
        "        for b in range(batch_size):#use the batch to forward\n",
        "          x = data[b]\n",
        "          y = label[b]\n",
        "          output = self.forward(x)\n",
        "          loss += cross_entropy(output, y)\n",
        "          if np.argmax(output) == np.argmax(y):\n",
        "              acc += 1\n",
        "              total_acc += 1\n",
        "          dy.append(y)\n",
        "        for y in dy:\n",
        "          self.backward(y)\n",
        "        # result\n",
        "        loss /= batch_size\n",
        "  \n",
        "      training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))      \n",
        "      valid_acc = self.predict(X_valid, y_valid)\n",
        "      self.acc.append(training_acc)\n",
        "      self.vacc.append(valid_acc)\n",
        "      print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} ====== TAcc: {4:.2f} ====== VAcc: {5:.2f} ======'.format(e,epoch,batch_index+batch_size,loss,training_acc, valid_acc))\n",
        "\n",
        "    if (weights_file is not None):\n",
        "      self.extract(weights_file)\n",
        "\n",
        "  def acc_plot(self):\n",
        "    epochs = range(1, len(self.acc) + 1)\n",
        "\n",
        "    # 畫出兩條線\n",
        "    plt.plot(epochs, self.acc, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, self.vacc, 'r', label='Validation accuracy')\n",
        "\n",
        "    # 設置標題、x軸和y軸的標籤\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n"
      ],
      "metadata": {
        "id": "wvVEuwMmFsSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Lenet5()\n",
        "net.train(train, train_labels, 128, 5)\n",
        "net.acc_plot\n",
        "print(net.predict(valid, valid_labels))\n",
        "print(net.predict(test, test_labels))"
      ],
      "metadata": {
        "id": "Inn8yls3wIyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet5_imp:\n",
        "  def __init__(self):\n",
        "    lr = 0.01\n",
        "    self.layers = []\n",
        "    self.layers.append(Conv2D(inputs_channel=1, num_filters=6, kernel_size=3, padding=0, stride=1, learning_rate=lr, name='conv1'))\n",
        "    self.layers.append(Conv2D(inputs_channel=6, num_filters=16, kernel_size=3, padding=0, stride=1, learning_rate=lr, name='conv2'))\n",
        "    self.layers.append(Sigmoid())\n",
        "    self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool3'))\n",
        "    self.layers.append(Conv2D(inputs_channel=16, num_filters=16, kernel_size=3, padding=0, stride=1, learning_rate=lr, name='conv4'))\n",
        "    self.layers.append(Sigmoid())\n",
        "    self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool5'))\n",
        "    self.layers.append(Conv2D(inputs_channel=16, num_filters=120, kernel_size=23, padding=0, stride=1, learning_rate=lr, name='conv6'))\n",
        "    self.layers.append(Sigmoid())\n",
        "    self.layers.append(Flatten())\n",
        "    self.layers.append(Dense(num_inputs=120, num_outputs=84, learning_rate=lr, name='fc7'))\n",
        "    self.layers.append(Sigmoid())\n",
        "    self.layers.append(Dense(num_inputs=84, num_outputs=50, learning_rate=lr, name='fc8'))\n",
        "    self.layers.append(Softmax())\n",
        "    self.lay_num = len(self.layers)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = x\n",
        "    for i in range(self.lay_num):\n",
        "      output = self.layers[i].forward(output)\n",
        "    return output\n",
        "\n",
        "  def backward(self, y):\n",
        "    for i in range(self.lay_num - 1, -1, -1):\n",
        "      y = self.layers[i].backward(y)\n",
        "    return y#actually not needed\n",
        "\n",
        "  def predict(self, data, label):\n",
        "    total_acc = 0\n",
        "    data_size = data.shape[0]\n",
        "    for i in range(data_size):\n",
        "      x = data[i]\n",
        "      y = label[i]\n",
        "      for l in range(self.lay_num):\n",
        "          output = self.layers[l].forward(x)\n",
        "          x = output\n",
        "      if np.argmax(output) == np.argmax(y):\n",
        "          total_acc += 1\n",
        "    return float(total_acc)\n",
        "    #print(\"Accuracy: \", float(total_acc)/float(data_size))\n",
        "\n",
        "  def extract(self, weights_file):\n",
        "    # dump weights and bias\n",
        "    obj = []\n",
        "    for i in range(self.lay_num):\n",
        "      cache = self.layers[i].extract()\n",
        "      obj.append(cache)\n",
        "    with open(weights_file, 'wb') as handle:\n",
        "      pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def train(self, training_data, training_label, batch_size, epoch, weights_file = None):\n",
        "    self.acc = []\n",
        "    self.vacc = []\n",
        "    total_acc = 0\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(training_data, training_label, test_size=0.2, stratify=training_label)\n",
        "\n",
        "    for e in range(epoch):\n",
        "      for batch_index in range(0, X_train.shape[0], batch_size):#taking datas <= batch size\n",
        "        if batch_index + batch_size < X_train.shape[0]:\n",
        "          data = X_train[batch_index:batch_index+batch_size]\n",
        "          label = training_label[batch_index:batch_index + batch_size]\n",
        "        else:\n",
        "          data = X_train[batch_index:X_train.shape[0]]\n",
        "          label = training_label[batch_index:training_label.shape[0]]\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        dy = []\n",
        "        for b in range(batch_size):#use the batch to forward\n",
        "          x = data[b]\n",
        "          y = label[b]\n",
        "          output = self.forward(x)\n",
        "          loss += cross_entropy(output, y)\n",
        "          if np.argmax(output) == np.argmax(y):\n",
        "              acc += 1\n",
        "              total_acc += 1\n",
        "          dy.append(y)\n",
        "        for y in dy:\n",
        "          self.backward(y)\n",
        "        # result\n",
        "        loss /= batch_size\n",
        "  \n",
        "      training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))      \n",
        "      valid_acc = self.predict(X_valid, y_valid)\n",
        "      self.acc.append(training_acc)\n",
        "      self.vacc.append(valid_acc)\n",
        "      print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} ====== TAcc: {4:.2f} ====== VAcc: {5:.2f} ======'.format(e,epoch,batch_index+batch_size,loss,training_acc, valid_acc))\n",
        "\n",
        "    if (weights_file is not None):\n",
        "      self.extract(weights_file)\n",
        "\n",
        "  def acc_plot(self):\n",
        "    epochs = range(1, len(self.acc) + 1)\n",
        "\n",
        "    # 畫出兩條線\n",
        "    plt.plot(epochs, self.acc, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, self.vacc, 'r', label='Validation accuracy')\n",
        "\n",
        "    # 設置標題、x軸和y軸的標籤\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n"
      ],
      "metadata": {
        "id": "Mx6puy8JWzc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Lenet5_imp()\n",
        "net2.train(train, train_labels, 128, 5)\n",
        "net2.acc_plot()\n",
        "print(net2.predict(valid, valid_labels))\n",
        "print(net2.predict(test, test_labels))"
      ],
      "metadata": {
        "id": "b5Z36hdzY59h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}